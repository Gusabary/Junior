# Homework

## Homework 1

1. huge page 可以看成是将下一级页表的所有 page 合并成了一个 page。对于 32 位机器来说，PTE 为 4B，所以一个页表页中有 1024 个 PTE，那么这 1024 个 page 合并起来就是 4M；而对于 64 位机器来说，PTE 为 8B，所以一个页表页中只有 512 个 PTE，一个 huge page 的大小也就是 2M。

2. 用时间换空间，多级页表的设计可以有效避免对很多没有用到的页的映射。

   一个四级页表页 spatial overhead 最大值为 4088B，即整个页表页中只有一个 PTE 被用到。

3. PTE 中的第 0 位为 valid 位，表示该项是否有效。

4. 好处：减少页表级数，提升遍历页表的效率。当应用程序一次申请多个页的时候可以分配给它一张大页。

   弊端：粒度比较粗，使用时容易造成空间上的 overhead。当应用程序逐渐释放之前申请的内存时，整张大页必须要等该页中全部内存都被释放才能回收。

5. 两个 ttbr 的好处在于应用程序和内核可以拥有不同的地址空间，如果不用这个机制，attribution-based isolation 确实可以起到一定的隔离作用，但是因为是作用在 PTE 上，所以粒度最小也就是一个页的大小。如果在某一个页中，部分内存是应用程序可访问的，而另一部分是应用程序不可访问的，attribution-based isolation 就很难解决这一点。

6. 因为不同进程的同一个虚拟地址可能对应不同的物理地址，所以进程切换的时候 TLB 需要刷新。而用户态切换到内核态的时候不需要刷新，因为它们用的是不同的 ttbr。

7. 将 PTE 中的 AP 设为 read-only，当试图进行写操作时会有异常，此时在 handler 中再将其设为 read/write。

## Homework 2

1. 只有当前线程被复制到子进程中。

   如果在 fork 前某个线程拿了锁，而 fork 之后因为只有当前线程，所以被另外一个线程持有的锁就不会被释放从而造成死锁。

   一个可能的原因是 fork 复制用户空间的虚存，而和线程相关的信息保存在内核空间中。

2. 因为即使采用 COW，fork 仍然需要先拷贝页表，而 vfork 连页表也不需要拷贝。

   使用更多的 huge page，这样在拷贝页表的时候可以降低拷贝的数量。

3. 因为跨进程的线程切换需要切换地址空间，这本身就是一个开销不小的操作，并且还可能导致 TLB 刷新。

4. 因为内核的虚存空间在高地址并且使用的 ttbr 也和用户态不一样，切换页表的时候不会对其造成影响。

5. 使用多进程 + 协程的方法可以利用多核资源，尤其适合需要频繁进行逻辑流调度的程序。

6. 对于一般的请求来说，创建一个新的线程，这样可以快速切换以满足低时延的要求；

   对于代码处理请求，创建一个新的进程，这样可以避免它访问到其他进程的虚存。

   TCB 中需要保存线程状态以及寄存器信息，PCB 中还需要保存进程的状态和虚存信息。

   在发生线程的上下文切换时，保存上下文、切换内核栈、恢复上下文即可；而在发生进程的上下文切换时，还需要切换页表。

## Homework 3

1. 父子进程间利用共享内存通信

   Unix I/O 模型中的异步 I/O

   Unix I/O 模型中的阻塞式 I/O

   对某个进程组发送 signal

2. 同步对 `struct pipe` 的关键操作，例如修改其中的 data 字段

   因为持锁状态下不能 sleep，而从 sleep 中被唤醒之后继续执行需要先拿锁

   因为 wakeup 虽然会唤醒进程，但并不意味着被唤醒的进程会立刻调度上来，所以如果在 wakeup 中就拿锁的话会延长不必要的持锁时间

3. 即检查某个信息和使用这个信息这两个操作不是原子的，之间存在时间间隔。对于共享内存来说，有可能出现两个进程同时写某块内存的情况。

   一个解决方法是在进行地址翻译前对共享内存加锁。

4. 参数栈用来存放参数，需要同时映射到 caller 和 callee 两个进程的地址空间中，而执行栈作为运行时的栈只需要映射到 callee 进程的地址空间中。

   将 caller 的返回地址和栈指针存放到 linkage record 中并将其放到 TCB 的栈上。

   如果参数是指针的话，被指向的资源需要做额外处理才能在两个进程之间共享，否则有可能出现段错误或者访问到其他数据。

5. 应该是会的，IPC 性能很大一部分取决于目标进程是不是被立刻调度上来，如果 naming mechanism 的设计能尽量保证这一点的话，IPC 性能会有提升。

6. ChCore 中的 IPC 需要将 ipc_msg 在客户进程的虚拟地址转换到服务进程中的虚拟地址，走页表的开销很大，如果能通过硬件实现直接定位到最后一级页表，省去走前几级页表的开销，IPC 的性能会有提升。

## Homework 4

1. 因为自旋锁在单核场景下没有意义，当持锁的进程 A 被调度下去之后，另一个申请锁的进程 B 的整个时间片都用来检查锁的状态，必须要等 A 放锁了 B 才能拿到，而 B 在运行着的时候 A 又是放不了锁的。

   但是在多核场景下，进程 A，B 可以在两个不同的核上运行，也就意味着 B 运行的过程中 A 有可能把锁放了，这样 B 就能立刻拿到锁。

   要在单核场景下使用的话，需要让进程 B 申请锁而不得后去睡觉，避免浪费时间片。

2. 因为只要有 reader 不断进来，writer 就不可能拿到锁，哪怕这些 reader 是在 writer 之后进来的。

   偏向 writer 的读写锁的一个设计是只有在 writer 前进来的 reader 可以拿到锁，之后的 reader 要等 writer 拿锁并放锁后才能拿锁。

3. 将处理器和资源抽象成两类不同的结点（处理器为 A 类，资源为 B 类），由 A 类节点指向 B 类节点的边表示某处理器正在申请某资源，由 B 类节点指向 A 类节点的边表示某处理器持有某资源。可以画出一张图，其中有四个 A 类节点，以及五个 B 类节点，又因为对于每个 A 类节点，其入度与出度之和不能超过 2，所以该图中不存在环（五个 B 类节点是等价的，A 类节点可以与其中任意一个相连），即不存在死锁。

4. 对于 `lock->owner` 的高频竞争导致严重的性能开销。

   后来的 ticket 不再竞争 `lock->owner`，而是等前一个 ticket 通知自己。

5. 因为一旦某个 NUMA 节点拿到了全局锁，那么只要这个节点中还有竞争者，别的 NUMA 节点就一直拿不到锁，即使这些节点中早就有了竞争者。

   可以采用类似 round-robin 的机制，规定全局锁在一个 NUMA 节点内部传递过一定次数后就要放锁给别的节点。

6. ```c
   void lock(struct spinlock* lock) {
   	/* locking */
       barrier();
   }
   
   void unlock(struct spinlock* lock) {
       barrier();
   	/* unlocking */
   }
   ```

## Homework 5

1. 硬链接通过目录项指向同一个 inode 实现，软链接则是通过记录目标文件路径实现；

   硬链接会增加 reference counter，软链接不会；

   硬链接不能跨文件系统，软链接可以；

   目标文件被删除后硬链接仍然有效，软链接则会失效；

   硬链接不能对目录创建，软链接可以；

2. 如果允许对目录创建硬链接的话，可能会引入一个循环引用的问题。即目录中的某个硬链接指向这个目录，那么当删除这个目录时，由于有硬链接指向它，所以 reference counter 不为 0，inode 不会被释放，但是由于这个硬链接是这个目录下的，所以已经没有办法从根目录访问到这个目录，造成磁盘空间的泄露。

   一个解决方案时删除目录时先递归地删除该目录下所有文件，这样删除该目录时原先的硬链接已经被删除，就不会有循环引用的问题。和之前的实现相比，该做法无非是把递归删除目录下所有文件的操作提前了，所以性能一定会有所影响，至于复杂性，我觉得省去了之后找一个合适的时间做递归删除的操作，应该是更简单一些。

3. 正在被使用的 inode 被标记为空闲，导致数据被覆盖

4. durability：持久性，即操作在故障恢复后仍然可见；

   atomicity：原子性，即故障恢复后要么所有操作都可见，要么都不可见。

   可以用日志、写时复制保障这一点。

5. copy-on-write 向上递归复制和修改，直到所有修改能用一个原子操作完成

   journaling 在磁盘上记录日志，如果崩溃发生在 commit 之后，那么所有操作可以恢复

   log-structured updates 假设文件被缓存在内存中，读请求可以被很好地处理，所以以顺序写入的方式记录文件系统的修改

   journaling 相较于 copy-on-write，更适合用于两处修改位置的“共同祖先”靠下的情况，这样可以少进行几次向上递归。log-structured updates 的话有一个文件缓存在内存中的假设，但很多时候会真的去磁盘上读。

6. 尽量减少写操作并将写操作尽可能地分摊到整个设备，如使用 NAT 优化递归更新。

   将 block 标记为无效块。

## Homework 6

1. LDDM 有 Device, Bus 以及 Class 三种抽象，分别对应 `/sys` 目录下的三个子目录。

   + Device 用于抽象系统中的所有硬件。
   + Bus 则是 CPU 连接 Device 的通道。
   + Class 是一组具有相似功能或属性的设备集合。

2. 优先级重置相当于上半部，确认中断源并解除对低优先级中断的屏蔽；中断无效相当于下半部，完全处理完后通知 GIC。这样可以提高系统响应中断的实时性。

3. 内核中接收队列被填满，新来的包只能被丢弃然后重传，最终导致活锁。解决方法是提高操作系统对包的处理速度。

4. 网卡收到数据包以后用 DMA 将其传送至 rx_ring，然后触发中断。中断的上半部将一个 skb 包入队，下半部将其交给对应的协议栈处理，最后数据拷贝到用户空间。

   Linux 需要一次上下文切换，ChCore 需要两次。

5. skb 本身不存储报文，而是通过指针指向报文真正的内存空间，在各层传递时只需调整对应的指针位置即可。

   使用 tcpdump 等抓包工具时需要浅拷贝；修改报文时（例如 NAT）需要深拷贝。

6. 抛弃中断，使用轮询模式驱动；控制平面与数据平面相分离，在用户态访问 MMIO 和 ring buffers。

   可以使用大页，因为微内核场景下上下文切换比较频繁，减少 TLB miss 能有效提升 network server 的性能。

## Homework 7

1. 需要 400 个 shadow page table，需要 20 个 stage-2 page table。

2. TLB miss 的时候，stage-2 page table 最多需要进行 24 次访存，性能大大下降。

3. 防止 DMA 恶意读写别的 VM 的内存。

   GPA 和 HPA。SMMU 先将 IOVA 翻译成 GPA，再将 GPA 翻译成 HPA。

4. 安全级别从低到高应该是 container，AWS Firecracker，gVisor，traditional VM。

   AWS Firecracker 保留 KVM，但只实现了运行函数的必要功能；gVisor 是用户态内核，提供与虚拟机相当的强隔离性。

5. 由于微内核的架构，需要在用户态模块中实现 container support（比如增强原有文件系统模块的代码，或者新增一个负责 container support 的模块）。性能上新增模块的话会增加 ipc 的次数，但是因为新增的代码大部分在用户态，所以仍然可以尽可能地保证内核的安全性。

## Homework 8

1. + 保护重要数据，仅允许它们被合法访问；
   + 与恶意应用作斗争，保护自己、限制对方；
   + 考虑操作系统被完全攻破的情况下依然提供一定的保护。
2. capability 是 access control 的实现方法之一，从实体角度出发，列出该实体所拥有的能访问的对象及相应的权限。
3. enclave 威胁模型是说不信任 CPU 以外的硬件。因为数据需要流动才能产生更大的价值，而数据太容易复制，导致不可控。因为 enclave 仍然需要操作系统提供的服务。
4. 需要，因为要确保极端状况下操作系统仍能正常运行。
5. 可以从底层构造输入，比如不停地收网络包，或者不停地接收键盘输入。

##### Last-modified date: 2020.6.13, 3 p.m.

