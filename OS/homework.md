# Homework

## Homework 1

1. huge page 可以看成是将下一级页表的所有 page 合并成了一个 page。对于 32 位机器来说，PTE 为 4B，所以一个页表页中有 1024 个 PTE，那么这 1024 个 page 合并起来就是 4M；而对于 64 位机器来说，PTE 为 8B，所以一个页表页中只有 512 个 PTE，一个 huge page 的大小也就是 2M。

2. 用时间换空间，多级页表的设计可以有效避免对很多没有用到的页的映射。

   一个四级页表页 spatial overhead 最大值为 4088B，即整个页表页中只有一个 PTE 被用到。

3. PTE 中的第 0 位为 valid 位，表示该项是否有效。

4. 好处：减少页表级数，提升遍历页表的效率。当应用程序一次申请多个页的时候可以分配给它一张大页。

   弊端：粒度比较粗，使用时容易造成空间上的 overhead。当应用程序逐渐释放之前申请的内存时，整张大页必须要等该页中全部内存都被释放才能回收。

5. 两个 ttbr 的好处在于应用程序和内核可以拥有不同的地址空间，如果不用这个机制，attribution-based isolation 确实可以起到一定的隔离作用，但是因为是作用在 PTE 上，所以粒度最小也就是一个页的大小。如果在某一个页中，部分内存是应用程序可访问的，而另一部分是应用程序不可访问的，attribution-based isolation 就很难解决这一点。

6. 因为不同进程的同一个虚拟地址可能对应不同的物理地址，所以进程切换的时候 TLB 需要刷新。而用户态切换到内核态的时候不需要刷新，因为它们用的是不同的 ttbr。

7. 将 PTE 中的 AP 设为 read-only，当试图进行写操作时会有异常，此时在 handler 中再将其设为 read/write。

## Homework 2

1. 只有当前线程被复制到子进程中。

   如果在 fork 前某个线程拿了锁，而 fork 之后因为只有当前线程，所以被另外一个线程持有的锁就不会被释放从而造成死锁。

   一个可能的原因是 fork 复制用户空间的虚存，而和线程相关的信息保存在内核空间中。

2. 因为即使采用 COW，fork 仍然需要先拷贝页表，而 vfork 连页表也不需要拷贝。

   使用更多的 huge page，这样在拷贝页表的时候可以降低拷贝的数量。

3. 因为跨进程的线程切换需要切换地址空间，这本身就是一个开销不小的操作，并且还可能导致 TLB 刷新。

4. 因为内核的虚存空间在高地址并且使用的 ttbr 也和用户态不一样，切换页表的时候不会对其造成影响。

5. 使用多进程 + 协程的方法可以利用多核资源，尤其适合需要频繁进行逻辑流调度的程序。

6. 对于一般的请求来说，创建一个新的线程，这样可以快速切换以满足低时延的要求；

   对于代码处理请求，创建一个新的进程，这样可以避免它访问到其他进程的虚存。

   TCB 中需要保存线程状态以及寄存器信息，PCB 中还需要保存进程的状态和虚存信息。

   在发生线程的上下文切换时，保存上下文、切换内核栈、恢复上下文即可；而在发生进程的上下文切换时，还需要切换页表。

## Homework 3

1. 父子进程间利用共享内存通信

   Unix I/O 模型中的异步 I/O

   Unix I/O 模型中的阻塞式 I/O

   对某个进程组发送 signal

2. 同步对 `struct pipe` 的关键操作，例如修改其中的 data 字段

   因为持锁状态下不能 sleep，而从 sleep 中被唤醒之后继续执行需要先拿锁

   因为 wakeup 虽然会唤醒进程，但并不意味着被唤醒的进程会立刻调度上来，所以如果在 wakeup 中就拿锁的话会延长不必要的持锁时间

3. 即检查某个信息和使用这个信息这两个操作不是原子的，之间存在时间间隔。对于共享内存来说，有可能出现两个进程同时写某块内存的情况。

   一个解决方法是在进行地址翻译前对共享内存加锁。

4. 参数栈用来存放参数，需要同时映射到 caller 和 callee 两个进程的地址空间中，而执行栈作为运行时的栈只需要映射到 callee 进程的地址空间中。

   将 caller 的返回地址和栈指针存放到 linkage record 中并将其放到 TCB 的栈上。

   如果参数是指针的话，被指向的资源需要做额外处理才能在两个进程之间共享，否则有可能出现段错误或者访问到其他数据。

5. 应该是会的，IPC 性能很大一部分取决于目标进程是不是被立刻调度上来，如果 naming mechanism 的设计能尽量保证这一点的话，IPC 性能会有提升。

6. ChCore 中的 IPC 需要将 ipc_msg 在客户进程的虚拟地址转换到服务进程中的虚拟地址，走页表的开销很大，如果能通过硬件实现直接定位到最后一级页表，省去走前几级页表的开销，IPC 的性能会有提升。

## Homework 4

1. 因为自旋锁在单核场景下没有意义，当持锁的进程 A 被调度下去之后，另一个申请锁的进程 B 的整个时间片都用来检查锁的状态，必须要等 A 放锁了 B 才能拿到，而 B 在运行着的时候 A 又是放不了锁的。

   但是在多核场景下，进程 A，B 可以在两个不同的核上运行，也就意味着 B 运行的过程中 A 有可能把锁放了，这样 B 就能立刻拿到锁。

   要在单核场景下使用的话，需要让进程 B 申请锁而不得后去睡觉，避免浪费时间片。

2. 因为只要有 reader 不断进来，writer 就不可能拿到锁，哪怕这些 reader 是在 writer 之后进来的。

   偏向 writer 的读写锁的一个设计是只有在 writer 前进来的 reader 可以拿到锁，之后的 reader 要等 writer 拿锁并放锁后才能拿锁。

3. 将处理器和资源抽象成两类不同的结点（处理器为 A 类，资源为 B 类），由 A 类节点指向 B 类节点的边表示某处理器正在申请某资源，由 B 类节点指向 A 类节点的边表示某处理器持有某资源。可以画出一张图，其中有四个 A 类节点，以及五个 B 类节点，又因为对于每个 A 类节点，其入度与出度之和不能超过 2，所以该图中不存在环（五个 B 类节点是等价的，A 类节点可以与其中任意一个相连），即不存在死锁。

4. 对于 `lock->owner` 的高频竞争导致严重的性能开销。

   后来的 ticket 不再竞争 `lock->owner`，而是等前一个 ticket 通知自己。

5. 因为一旦某个 NUMA 节点拿到了全局锁，那么只要这个节点中还有竞争者，别的 NUMA 节点就一直拿不到锁，即使这些节点中早就有了竞争者。

   可以采用类似 round-robin 的机制，规定全局锁在一个 NUMA 节点内部传递过一定次数后就要放锁给别的节点。

6. ```c
   void lock(struct spinlock* lock) {
   	/* locking */
       barrier();
   }
   
   void unlock(struct spinlock* lock) {
       barrier();
   	/* unlocking */
   }
   ```

##### Last-modified date: 2020.4.23, 9 a.m.

